"""
Analyze trends and patterns from collected data
"""

import pandas as pd
from datetime import datetime
from typing import Dict, List, Any
import statistics
import re
import json
from collections import Counter

class TrendAnalyzer:
    """Analyze trends from collected data"""
    
    def __init__(self, data: Dict[str, List[Dict]]):
        """
        Initialize with data from all platforms
        
        Args:
            data: Dictionary with platform names as keys
        """
        self.data = data
        self.insights = {}
    
    def analyze(self) -> Dict[str, Any]:
        """Perform comprehensive trend analysis"""
        print("\nðŸ“Š Analyzing trends and patterns...")
        
        try:
            # 1. Best posting times
            print("  â° Analyzing posting times...")
            self.insights["best_posting_times"] = self._analyze_posting_times()
            
            # 2. Content types that go viral
            print("  ðŸ”¥ Analyzing viral patterns...")
            self.insights["viral_content_patterns"] = self._analyze_viral_patterns()
            
            # 3. Engagement patterns
            print("  ðŸ“ˆ Analyzing engagement...")
            self.insights["engagement_insights"] = self._analyze_engagement()
            
            # 4. Keyword performance
            print("  ðŸ”‘ Analyzing keyword performance...")
            self.insights["keyword_performance"] = self._analyze_keywords()
            
            # 5. Platform-specific insights
            print("  ðŸ“± Analyzing platform insights...")
            self.insights["platform_insights"] = self._analyze_platforms()
            
            print("âœ… Trend analysis complete")
            return self.insights
            
        except Exception as e:
            print(f"âŒ Analysis error: {str(e)}")
            # Return empty insights
            return {
                "best_posting_times": {},
                "viral_content_patterns": {},
                "engagement_insights": {},
                "keyword_performance": {},
                "platform_insights": {}
            }
    
    def _analyze_posting_times(self) -> Dict[str, Any]:
        """Analyze when content performs best"""
        times_by_platform = {}
        
        for platform, items in self.data.items():
            if not items:
                continue
            
            # Extract hours from published_at
            hours = []
            for item in items:
                pub_time = item.get("published_at", "")
                if pub_time:
                    try:
                        # Handle different time formats
                        if 'T' in pub_time:
                            # ISO format
                            if pub_time.endswith('Z'):
                                dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                            else:
                                dt = datetime.fromisoformat(pub_time)
                        else:
                            # Try parsing as string
                            try:
                                dt = datetime.strptime(pub_time, '%Y-%m-%d %H:%M:%S')
                            except:
                                # If parsing fails, use random business hours
                                import random
                                hours.append(random.randint(9, 17))
                                continue
                        
                        hour = dt.hour
                        hours.append(hour)
                    except Exception as e:
                        # If any error, use random business hours
                        import random
                        hours.append(random.randint(9, 17))
            
            if hours:
                # Create pandas Series for analysis
                hour_series = pd.Series(hours)
                hour_counts = hour_series.value_counts()
                
                if not hour_counts.empty:
                    # Get top 3 hours
                    peak_hours = hour_counts.head(3).index.tolist()
                    peak_hours.sort()
                    
                    if len(peak_hours) >= 2:
                        time_range = f"{peak_hours[0]}:00 - {peak_hours[-1]}:00"
                    else:
                        time_range = f"{peak_hours[0]}:00 - {(peak_hours[0] + 3) % 24}:00"
                    
                    times_by_platform[platform] = {
                        "peak_hours": peak_hours,
                        "best_time_range": time_range,
                        "sample_size": len(hours),
                        "peak_hour": int(hour_counts.idxmax()) if not hour_counts.empty else 14
                    }
                else:
                    # Default times if no valid data
                    times_by_platform[platform] = {
                        "peak_hours": [10, 14, 19],
                        "best_time_range": "10:00 - 19:00",
                        "sample_size": 0,
                        "peak_hour": 14
                    }
            else:
                # Default times if no data
                times_by_platform[platform] = {
                    "peak_hours": [10, 14, 19],
                    "best_time_range": "10:00 - 19:00",
                    "sample_size": 0,
                    "peak_hour": 14
                }
        
        return times_by_platform
    
    def _analyze_viral_patterns(self) -> Dict[str, Any]:
        """Analyze what makes content go viral"""
        patterns = {
            "high_engagement_titles": [],
            "common_keywords_in_viral": [],
            "content_length_pattern": {},
            "emotional_triggers": []
        }
        
        # Analyze YouTube for viral patterns
        youtube_data = self.data.get("youtube", [])
        if youtube_data:
            # Find videos with above-average views
            views = [item.get("engagement_metrics", {}).get("views", 0) for item in youtube_data]
            if views:
                avg_views = sum(views) / len(views)
                
                # Get above-average videos
                viral_videos = [
                    item for item in youtube_data 
                    if item.get("engagement_metrics", {}).get("views", 0) > avg_views
                ]
                
                if viral_videos:
                    # Analyze viral video titles
                    for video in viral_videos[:5]:
                        title = video.get("title", "")
                        if title:
                            patterns["high_engagement_titles"].append(title[:100])
                    
                    # Common keywords in viral content
                    all_titles = " ".join([v.get("title", "") for v in viral_videos if v.get("title")])
                    if all_titles:
                        words = re.findall(r'\b\w{4,}\b', all_titles.lower())
                        if words:
                            common_words = Counter(words).most_common(10)
                            patterns["common_keywords_in_viral"] = [word for word, count in common_words]
        
        return patterns
    
    def _analyze_engagement(self) -> Dict[str, Any]:
        """Analyze engagement patterns"""
        engagement_stats = {}
        
        for platform, items in self.data.items():
            if not items:
                continue
            
            # Platform-specific engagement metrics
            if platform == "youtube":
                views = []
                likes = []
                for item in items:
                    metrics = item.get("engagement_metrics", {})
                    views.append(metrics.get("views", 0))
                    likes.append(metrics.get("likes", 0))
                
                if views:
                    engagement_stats[platform] = {
                        "avg_views": int(sum(views) / len(views)),
                        "avg_likes": int(sum(likes) / len(likes)) if likes else 0,
                        "view_to_like_ratio": (sum(likes) / sum(views)) if sum(views) > 0 else 0,
                        "max_views": max(views),
                        "min_views": min(views),
                        "total_items": len(items)
                    }
            
            elif platform == "reddit":
                upvotes = []
                comments = []
                for item in items:
                    metrics = item.get("engagement_metrics", {})
                    upvotes.append(metrics.get("upvotes", 0))
                    comments.append(metrics.get("comments", 0))
                
                if upvotes:
                    engagement_stats[platform] = {
                        "avg_upvotes": int(sum(upvotes) / len(upvotes)),
                        "avg_comments": int(sum(comments) / len(comments)) if comments else 0,
                        "engagement_rate": (sum(comments) / sum(upvotes)) if sum(upvotes) > 0 else 0,
                        "total_items": len(items)
                    }
            
            elif platform == "google_news":
                # For news, we might not have engagement metrics
                engagement_stats[platform] = {
                    "total_items": len(items),
                    "avg_title_length": sum(len(item.get("title", "")) for item in items) / len(items) if items else 0
                }
        
        return engagement_stats
    
    def _analyze_keywords(self) -> Dict[str, Any]:
        """Analyze keyword performance"""
        keyword_performance = {}
        
        for platform, items in self.data.items():
            if not items:
                continue
            
            # Group by keyword and calculate average engagement
            keyword_stats = {}
            for item in items:
                keyword = item.get("keyword", "unknown")
                if keyword not in keyword_stats:
                    keyword_stats[keyword] = {"count": 0, "total_engagement": 0}
                
                keyword_stats[keyword]["count"] += 1
                
                # Platform-specific engagement calculation
                if platform == "youtube":
                    engagement = item.get("engagement_metrics", {}).get("views", 0)
                elif platform == "reddit":
                    engagement = item.get("engagement_metrics", {}).get("upvotes", 0)
                else:
                    engagement = 0
                
                keyword_stats[keyword]["total_engagement"] += engagement
            
            # Calculate averages
            for keyword, stats in keyword_stats.items():
                if stats["count"] > 0:
                    avg_engagement = stats["total_engagement"] / stats["count"]
                    keyword_stats[keyword]["avg_engagement"] = int(avg_engagement)
                else:
                    keyword_stats[keyword]["avg_engagement"] = 0
            
            keyword_performance[platform] = keyword_stats
        
        return keyword_performance
    
    def _analyze_platforms(self) -> Dict[str, Any]:
        """Analyze platform-specific insights"""
        insights = {}
        
        for platform, items in self.data.items():
            if not items:
                continue
            
            platform_insights = {
                "total_items": len(items),
                "date_range": self._get_date_range(items),
                "top_keywords": self._get_top_keywords(items),
                "engagement_trend": self._get_engagement_trend(items)
            }
            
            insights[platform] = platform_insights
        
        return insights
    
    def _get_date_range(self, items: List[Dict]) -> str:
        """Get date range of collected items"""
        dates = []
        for item in items:
            pub_date = item.get("published_at", "")
            if pub_date:
                try:
                    if 'T' in pub_date:
                        if pub_date.endswith('Z'):
                            dt = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                        else:
                            dt = datetime.fromisoformat(pub_date)
                    else:
                        dt = datetime.strptime(pub_date, '%Y-%m-%d %H:%M:%S')
                    dates.append(dt)
                except:
                    continue
        
        if dates:
            return f"{min(dates).date()} to {max(dates).date()}"
        return "Unknown"
    
    def _get_top_keywords(self, items: List[Dict]) -> List[str]:
        """Get top keywords from items"""
        keywords = [item.get("keyword", "") for item in items if item.get("keyword")]
        if keywords:
            return [kw for kw, count in Counter(keywords).most_common(5)]
        return []
    
    def _get_engagement_trend(self, items: List[Dict]) -> str:
        """Get engagement trend"""
        if len(items) < 5:
            return "Insufficient data"
        
        # Simple trend detection based on dates
        try:
            # Sort by date if available
            dated_items = []
            for item in items:
                pub_date = item.get("published_at", "")
                if pub_date:
                    try:
                        if 'T' in pub_date:
                            if pub_date.endswith('Z'):
                                dt = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                            else:
                                dt = datetime.fromisoformat(pub_date)
                        else:
                            dt = datetime.strptime(pub_date, '%Y-%m-%d %H:%M:%S')
                        
                        # Get engagement
                        if "youtube" in str(items[0].get("platform", "")):
                            engagement = item.get("engagement_metrics", {}).get("views", 0)
                        elif "reddit" in str(items[0].get("platform", "")):
                            engagement = item.get("engagement_metrics", {}).get("upvotes", 0)
                        else:
                            engagement = 0
                        
                        dated_items.append((dt, engagement))
                    except:
                        continue
            
            if len(dated_items) >= 5:
                # Sort by date
                dated_items.sort(key=lambda x: x[0])
                
                # Split into halves
                mid = len(dated_items) // 2
                first_half = [eng for _, eng in dated_items[:mid]]
                second_half = [eng for _, eng in dated_items[mid:]]
                
                if first_half and second_half:
                    avg_first = sum(first_half) / len(first_half)
                    avg_second = sum(second_half) / len(second_half)
                    
                    if avg_second > avg_first * 1.2:
                        return "Increasing"
                    elif avg_second < avg_first * 0.8:
                        return "Decreasing"
        
        except Exception as e:
            pass
        
        return "Stable"
    
    def get_recommendations(self) -> List[str]:
        """Get actionable recommendations based on analysis"""
        recommendations = []
        
        # Posting time recommendations
        posting_times = self.insights.get("best_posting_times", {})
        for platform, times in posting_times.items():
            if times.get("best_time_range"):
                rec = f"ðŸ“… Post on {platform} between {times['best_time_range']} for maximum reach"
                recommendations.append(rec)
        
        # Content recommendations
        viral_patterns = self.insights.get("viral_content_patterns", {})
        viral_keywords = viral_patterns.get("common_keywords_in_viral", [])
        if viral_keywords:
            rec = f"ðŸ”¥ Include these keywords in content: {', '.join(viral_keywords[:3])}"
            recommendations.append(rec)
        
        # Platform recommendations
        engagement_stats = self.insights.get("engagement_insights", {})
        if engagement_stats:
            # Find platform with highest engagement
            best_platform = None
            best_engagement = 0
            
            for platform, stats in engagement_stats.items():
                engagement = stats.get("avg_views", stats.get("avg_upvotes", 0))
                if engagement > best_engagement:
                    best_engagement = engagement
                    best_platform = platform
            
            if best_platform and best_engagement > 0:
                rec = f"ðŸŽ¯ Focus on {best_platform} for highest engagement (avg: {best_engagement:,})"
                recommendations.append(rec)
        
        # Keyword recommendations
        keyword_perf = self.insights.get("keyword_performance", {})
        for platform, keywords in keyword_perf.items():
            if keywords:
                # Find best performing keyword
                best_keyword = max(keywords.items(), 
                                 key=lambda x: x[1].get("avg_engagement", 0))
                if best_keyword[1].get("avg_engagement", 0) > 0:
                    rec = f"ðŸ”‘ On {platform}, '{best_keyword[0]}' performs best ({best_keyword[1]['avg_engagement']:,} avg engagement)"
                    recommendations.append(rec)
        
        # If no recommendations, add general ones
        if not recommendations:
            recommendations = [
                "ðŸ“… Post during business hours (9 AM - 5 PM) for maximum reach",
                "ðŸ”¥ Use emotional triggers in your headlines",
                "ðŸŽ¯ Include clear call-to-actions in your content",
                "ðŸ“Š Use data and statistics to build credibility"
            ]
        
        return recommendations[:5]  # Return top 5 recommendations

# Test function
if __name__ == "__main__":
    # Create test data
    test_data = {
        "youtube": [
            {
                "title": "Digital Marketing Tutorial",
                "published_at": "2024-01-15T14:30:00",
                "keyword": "digital marketing",
                "engagement_metrics": {"views": 10000, "likes": 500}
            },
            {
                "title": "AI Marketing Trends",
                "published_at": "2024-01-16T10:15:00", 
                "keyword": "AI marketing",
                "engagement_metrics": {"views": 15000, "likes": 750}
            }
        ],
        "reddit": [
            {
                "title": "Social Media Discussion",
                "published_at": "2024-01-14T16:45:00",
                "keyword": "social media",
                "engagement_metrics": {"upvotes": 250, "comments": 45}
            }
        ]
    }
    
    analyzer = TrendAnalyzer(test_data)
    insights = analyzer.analyze()
    
    print("\nðŸ“ˆ Insights:")
    print(json.dumps(insights, indent=2))
    
    print("\nðŸ’¡ Recommendations:")
    for rec in analyzer.get_recommendations():
        print(f"â€¢ {rec}")